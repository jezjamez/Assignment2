{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a962dd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9939599",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['bicycle','trailer','caravan', 'truck', 'motorcycle', 'car', 'vehicle fallback', 'rider', 'bus', 'autorickshaw', 'person', 'animal', 'traffic sign', 'train', 'traffic light']\n",
    "train_info_file = '/work/Assignment2/dataset_info/test.txt'\n",
    "val_info_file = '/work/Assignment2/dataset_info/train.txt'\n",
    "labels_path = '/work/Assignment2/dataset_info/val.txt'\n",
    "annotations_path = '/work/Detection/idd-detection/IDD_Detection/Annotations/'\n",
    "old_images_path = '/work/Detection/idd-detection/IDD_Detection/JPEGImages/'\n",
    "images_path = '/work/Assignment2/Images/'\n",
    "model_dir = \"/work/Assignment2/Models\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99ca1a5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: '/work/Assignment2/dataset_info/val.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(train_info_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     train_img_paths \u001b[38;5;241m=\u001b[39m [line\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/os.py:223\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 223\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: '/work/Assignment2/dataset_info/val.txt'"
     ]
    }
   ],
   "source": [
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(labels_path, exist_ok=True)\n",
    "with open(train_info_file, 'r') as f:\n",
    "    train_img_paths = [line.strip() for line in f]\n",
    "f.close()\n",
    "with open(val_info_file, 'r') as f:\n",
    "    val_img_paths = [line.strip() for line in f]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cb5408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(size, box, scale_x, scale_y):\n",
    "    dw = (float(1.0) / (size[0])) \n",
    "    dh = (float(1.0)/ (size[1])) \n",
    "    box0 = max(float(1.0), box[0])\n",
    "    box2 = max(float(1.0), box[2])\n",
    "    box1 = min(size[0] - 1.0, box[1])\n",
    "    box3 = min(size[1] - 1.0, box[3])\n",
    "        \n",
    "    x = ((box0 + box1) / float(2.0) ) \n",
    "    y = ((box2 + box3) / float(2.0) ) \n",
    "    w = max((box1 - box0), float(1.0)) \n",
    "    h = max((box3 - box2), float(1.0))  \n",
    "    x *= dw\n",
    "    w *= dw\n",
    "    y *= dh\n",
    "    h *= dh  \n",
    "    return (x, y, w, h)\n",
    "\n",
    "def convert_labels(annotations_path, labels_path, annotations):\n",
    "    for annotation in annotations:\n",
    "        folder_path = annotation.rsplit('/', 1)[0]\n",
    "        os.makedirs(labels_path + '/' + folder_path, exist_ok=True)\n",
    "        in_file = open(annotations_path + '/' + annotation + '.xml')\n",
    "        out_file = open(labels_path + '/' + annotation + '.txt', 'w')\n",
    "\n",
    "        tree = ET.parse(in_file)\n",
    "        root = tree.getroot()\n",
    "        size = root.find('size')\n",
    "        w = int(size.find('width').text)\n",
    "        h = int(size.find('height').text)\n",
    "        scale_x = float(416.0) / float(w)\n",
    "        scale_y = float(416.0) / float(h)\n",
    "#         image =cv2.imread(old_images_path + '/' + annotation + '.jpg')\n",
    "#         image = cv2.resize(src=image, dsize=(416, 416))\n",
    "#         os.makedirs(images_path + '/' + folder_path, exist_ok=True)\n",
    "#         cv2.imwrite(images_path + '/' + annotation + '.jpg', image)\n",
    "        \n",
    "        \n",
    "        for obj in root.iter('object'):\n",
    "            cls = obj.find('name').text\n",
    "\n",
    "#             if cls not in classes:\n",
    "#                 continue\n",
    "\n",
    "            cls_idx = classes.index(cls)\n",
    "            box = obj.find('bndbox')\n",
    "            b = (float(box.find('xmin').text),\n",
    "                 float(box.find('xmax').text),\n",
    "                 float(box.find('ymin').text),\n",
    "                 float(box.find('ymax').text))\n",
    "            \n",
    "            bbox = convert((w, h), b , scale_x, scale_y)\n",
    "            out_file.write(str(cls_idx) + \" \" + \n",
    "                           \" \".join([str(bb) for bb in bbox]) + '\\n')\n",
    "\n",
    "        in_file.close()\n",
    "        out_file.close()\n",
    "    return classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = train_img_paths + val_img_paths\n",
    "# convert_labels(annotations_path, labels_path, annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9315243f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#   ! pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96773b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install opencv-python==4.5.5.64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ce04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  !pip install PyQt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e052f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ! pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded8cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations \n",
    "import cv2\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5187a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.LongestMaxSize(max_size=int(img_size * 1.1)),\n",
    "        albumentations.PadIfNeeded(\n",
    "            min_height=int(img_size * 1.1),\n",
    "            min_width=int(img_size * 1.1),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),\n",
    "        albumentations.RandomCrop(width=img_size, height=img_size),\n",
    "        albumentations.ColorJitter(brightness=0.6, contrast=0.6, \n",
    "                    saturation=0.6, hue=0.6, p=0.4),\n",
    "        albumentations.ShiftScaleRotate(\n",
    "            rotate_limit=20, p=0.5, border_mode=cv2.BORDER_CONSTANT\n",
    "        )\n",
    "        ,\n",
    "        albumentations.HorizontalFlip(p=0.5),\n",
    "        albumentations.Blur(p=0.1),\n",
    "        albumentations.CLAHE(p=0.1),\n",
    "        albumentations.Posterize(p=0.1),\n",
    "        albumentations.ToGray(p=0.1),\n",
    "        albumentations.ChannelShuffle(p=0.05),\n",
    "        albumentations.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format='yolo',\n",
    "                         min_visibility=0.4, \n",
    "                         label_fields=[]),\n",
    ")\n",
    "\n",
    "test_transforms = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.LongestMaxSize(max_size=int(img_size)), \n",
    "        albumentations.PadIfNeeded(\n",
    "            min_height=int(img_size),\n",
    "            min_width=int(img_size),\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "        ),  \n",
    "        albumentations.Normalize(mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255,),\n",
    "        ToTensorV2(),           \n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format='yolo',\n",
    "                             min_visibility=0.4, \n",
    "                             label_fields=[]),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd687900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import PyQt5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa407a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    intersection = (torch.min(box1[..., 0], box2[..., 0]) *\n",
    "                  torch.min(box1[..., 1], box2[..., 1]))\n",
    "\n",
    "    union = (box1[..., 0] * box1[..., 1] +\n",
    "            box2[..., 0] * box2[..., 1] -\n",
    "            intersection)\n",
    "\n",
    "    return intersection / (union + 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9a0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDDDataset(Dataset):\n",
    "    def __init__(self, annotations, image_dir, label_dir, anchors, image_size=416,\n",
    "                 strides=[13, 26, 52], classes=len(classes), transforms=None):\n",
    "        self.annotations = annotations\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.image_size = image_size\n",
    "        self.transforms = transforms\n",
    "        self.strides = strides\n",
    "        self.anchors = torch.tensor(anchors[0] + anchors[1] + anchors[2])\n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        self.num_anchors_per_scale = torch.div(self.num_anchors, 3, rounding_mode='floor')\n",
    "        self.classes = classes\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        label_path = os.path.join(self.label_dir, self.annotations[index] + '.txt')\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path, delimiter=' ', ndmin=2), 4, axis=1).tolist()\n",
    "        img_path = os.path.join(self.image_dir, self.annotations[index] + '.jpg')\n",
    "        image = np.array(Image.open(img_path).convert('RGB'))\n",
    "        if self.transforms:\n",
    "            augments = self.transforms(image=image, bboxes=bboxes)\n",
    "            image = augments['image']\n",
    "            bboxes = augments['bboxes']\n",
    "\n",
    "        targets = [torch.zeros((torch.div(self.num_anchors, 3, rounding_mode='floor'), S, S, 6)) for S in self.strides]\n",
    "        for box in bboxes:\n",
    "            \n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), self.anchors)\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, w, h, c = box\n",
    "            has_anchor = [False] * 3\n",
    "\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = torch.div(anchor_idx, self.num_anchors_per_scale, rounding_mode='floor')\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                S = self.strides[scale_idx]\n",
    "                i, j = int(S * y), int(S * x)\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "                    x_c, y_c = S * x - j, S * y - i\n",
    "                    w_c, h_c = (w * S, h * S)\n",
    "                    box_coordinates = torch.tensor([x_c, y_c, w_c, h_c])\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(c)\n",
    "                    has_anchor[scale_idx] = True\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "        return image, tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df909d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa67a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = nn.MSELoss()\n",
    "bce = nn.BCEWithLogitsLoss()\n",
    "entropy = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93ae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(box_pred, box_true, ciou=False):\n",
    "\n",
    "    box1_x1 = box_pred[..., 0:1] - box_pred[..., 2:3] / 2\n",
    "    box1_y1 = box_pred[..., 1:2] - box_pred[..., 3:4] / 2\n",
    "    box1_x2 = box_pred[..., 0:1] + box_pred[..., 2:3] / 2\n",
    "    box1_y2 = box_pred[..., 1:2] + box_pred[..., 3:4] / 2\n",
    "    box2_x1 = box_true[..., 0:1] - box_true[..., 2:3] / 2\n",
    "    box2_y1 = box_true[..., 1:2] - box_true[..., 3:4] / 2\n",
    "    box2_x2 = box_true[..., 0:1] + box_true[..., 2:3] / 2\n",
    "    box2_y2 = box_true[..., 1:2] + box_true[..., 3:4] / 2\n",
    "\n",
    "    x1 = torch.max(box1_x1, box2_x1)\n",
    "    y1 = torch.max(box1_y1, box2_y1)\n",
    "    x2 = torch.min(box1_x2, box2_x2)\n",
    "    y2 = torch.min(box1_y2, box2_y2)\n",
    "\n",
    "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
    "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
    "\n",
    "    return intersection / (box1_area + box2_area - intersection + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e81a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Loss(predictions, target, anchors):\n",
    "    obj = target[..., 0] == 1\n",
    "    noobj = target[..., 0] == 0 \n",
    "\n",
    "    no_object_loss = bce(\n",
    "        (predictions[..., 0:1][noobj]), (target[..., 0:1][noobj]),\n",
    "    )\n",
    "\n",
    "    anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "    box_preds = torch.cat([torch.sigmoid(predictions[..., 1:3]), torch.exp(predictions[..., 3:5]) * anchors], dim=-1)\n",
    "    ious = intersection_over_union(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "    object_loss = mse(torch.sigmoid(predictions[..., 0:1][obj]), ious * target[..., 0:1][obj])\n",
    "\n",
    "    predictions[..., 1:3] = torch.sigmoid(predictions[..., 1:3])\n",
    "    target[..., 3:5] = torch.log(\n",
    "        (1e-16 + target[..., 3:5] / anchors)\n",
    "    )\n",
    "    box_loss = mse(predictions[..., 1:5][obj], target[..., 1:5][obj])\n",
    "\n",
    "\n",
    "    class_loss = entropy(\n",
    "        (predictions[..., 5:][obj]), (target[..., 5][obj].long()),\n",
    "    )\n",
    "\n",
    "    return (10 * box_loss + 1 * object_loss + 10 * no_object_loss + 1 * class_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc999887",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_shapes = [\n",
    "    ('convolution', 32, 3, 1),      \n",
    "    ('convolution', 64, 3, 2), \n",
    "    ('residual', 1),\n",
    "    ('convolution', 128, 3, 2),\n",
    "    ('residual', 2),\n",
    "    ('convolution', 256, 3, 2),\n",
    "    ('residual', 8),\n",
    "    ('convolution', 512, 3, 2),\n",
    "    ('residual', 8),\n",
    "    ('convolution', 1024, 3, 2),\n",
    "    ('residual', 4),\n",
    "    ('convolution', 512, 1, 1),\n",
    "    ('convolution', 1024, 3, 1),\n",
    "    ('prediction'),\n",
    "    ('convolution', 256, 1, 1),\n",
    "    ('upsample'),\n",
    "    ('convolution', 256, 1, 1),\n",
    "    ('convolution', 512, 3, 1),\n",
    "    ('prediction'),\n",
    "    ('convolution', 128, 1, 1),\n",
    "    ('upsample'),\n",
    "    ('convolution', 128, 1, 1),\n",
    "    ('convolution', 256, 3, 1),\n",
    "    ('predicton'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c288655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017fd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, batch=True, act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, bias=not batch, **kwargs))\n",
    "\n",
    "        if batch:\n",
    "            layers.append(nn.BatchNorm2d(out_channels))\n",
    "        if act:\n",
    "            layers.append(nn.LeakyReLU(0.1, inplace=True))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    def __init__(self, in_channels, classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer = nn.Sequential(\n",
    "            Conv(in_channels, 2*in_channels, kernel_size=3, padding=1),\n",
    "            Conv(2*in_channels, (classes+5)*3,  batch=False, act=False, kernel_size=1),\n",
    "        )\n",
    "        self.classes = classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer(x)\n",
    "        x = x.reshape(x.shape[0], 3, self.classes+5, x.shape[2], x.shape[3])\n",
    "        x = x.permute(0, 1, 3, 4, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, channels, num_blocks, res=True):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        for _ in range(num_blocks):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    Conv(channels, torch.div(channels, 2, rounding_mode='floor'), kernel_size=1),\n",
    "                    Conv(torch.div(channels, 2, rounding_mode='floor'), channels, kernel_size=3, padding=1)\n",
    "                )\n",
    "            )\n",
    "        self.res = res\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.res:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "    \n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, num_of_classes ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.num_of_classes  = num_of_classes \n",
    "\n",
    "        self.layers = self.create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outs = []\n",
    "        route_conn = []\n",
    "\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Prediction):\n",
    "                outs.append(layer(x))\n",
    "                continue\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, Residual) and layer.num_blocks == 8:\n",
    "                route_conn.append(x)\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_conn.pop()], dim=1)\n",
    "        return outs\n",
    "\n",
    "    def create_model(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for shape in layer_shapes:\n",
    "            if shape[0] == 'convolution':\n",
    "                _, out_channels, kernel_size, stride = shape\n",
    "                padding = 0\n",
    "                if kernel_size == 3:\n",
    "                    padding = 1\n",
    "                layers.append(\n",
    "                    Conv(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=padding\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            if shape[0] == 'residual':\n",
    "                _, blocks = shape\n",
    "                layers.append(Residual(in_channels, blocks))\n",
    "\n",
    "            if shape[0] == 'u':\n",
    "                layers.append(nn.Upsample(scale_factor=2),)\n",
    "                in_channels = in_channels * 3\n",
    "            \n",
    "            if shape[0] == 'p':\n",
    "                layers += [\n",
    "                    Residual(in_channels, 1, res=False),\n",
    "                    Conv(in_channels, torch.div(in_channels, 2, rounding_mode='floor'), kernel_size=1),\n",
    "                    Prediction(torch.div(in_channels, 2, rounding_mode='floor'), self.num_of_classes )\n",
    "                ]\n",
    "                in_channels = torch.div(in_channels, 2, rounding_mode='floor')\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d96caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_channels  = 3\n",
    "num_of_classes = len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d8e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_channels, num_of_classes ).to(device)\n",
    "summary(model, (input_channels, 416, 416))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b63868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28424a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_threshold = 0.05\n",
    "def get_accuracy(true, pred, device):\n",
    "\n",
    "    total_c, correct_c = 0, 0\n",
    "    total_n, correct_n = 0, 0\n",
    "    total_o, correct_o = 0, 0\n",
    "\n",
    "    for i in range(3):\n",
    "        true[i] = true[i].to(device)\n",
    "        obj = true[i][..., 0] == 1\n",
    "        noobj = true[i][..., 0] == 0\n",
    "        correct_c += torch.sum(\n",
    "                torch.argmax(pred[i][..., 5][obj], dim=-1) == true[i][..., 5][obj]\n",
    "            )\n",
    "        total_c += torch.sum(obj)\n",
    "\n",
    "        obj_preds = torch.sigmoid(pred[i][..., 0]) > conf_threshold\n",
    "        correct_o += torch.sum(obj_preds[obj] == true[i][..., 0][obj])\n",
    "        total_o += torch.sum(obj)\n",
    "        correct_n += torch.sum(obj_preds[noobj] == true[i][..., 0][noobj])\n",
    "        total_n += torch.sum(noobj)\n",
    "\n",
    "    acc_c = correct_c / total_c * 100\n",
    "    acc_o = correct_o / total_o * 100\n",
    "    acc_n = correct_n / total_n * 100\n",
    "\n",
    "    return acc_c.item(), acc_o.item(), acc_n.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a55f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, train_loader, optimizer, loss, scaled_anchors, scaler, device='cuda'):\n",
    "    tq = tqdm(train_loader, leave=True, desc=\"Train\")\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, (data, target) in enumerate(tq):\n",
    "        data = data.to(device)\n",
    "        t0, t1, t2 = target[0].to(device), target[1].to(device), target[2].to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(data)\n",
    "\n",
    "            l = (loss(out[0], t0, scaled_anchors[0]) +\n",
    "                 loss(out[1], t1, scaled_anchors[1]) +\n",
    "                 loss(out[2], t2, scaled_anchors[2]))\n",
    "\n",
    "        losses.append(l.item())\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(l).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        l.detach()\n",
    "\n",
    "        mean_loss = sum(losses) / len(losses)\n",
    "        tq.set_postfix(loss=mean_loss)\n",
    "\n",
    "        acc = get_accuracy(target, out, device)\n",
    "        accuracy.append(acc)\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    avg_acc = np.array(accuracy).mean(axis=0)\n",
    "    return mean_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726a7828",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def val(model, val_loader, optimizer, loss, scaled_anchors, scaler, device='cuda'):\n",
    "    tq = tqdm(val_loader, leave=True, desc=\"Val\")\n",
    "    losses = []\n",
    "    accuracy = []\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(tq):\n",
    "            data = data.to(device)\n",
    "            t0, t1, t2 = target[0].to(device), target[1].to(device), target[2].to(device)\n",
    "            out = model(data)\n",
    "\n",
    "            l = (loss(out[0], t0, scaled_anchors[0]) +\n",
    "                 loss(out[1], t1, scaled_anchors[1]) +\n",
    "                 loss(out[2], t2, scaled_anchors[2]))\n",
    "\n",
    "            losses.append(l.detach().item())\n",
    "\n",
    "            mean_loss = sum(losses) / len(losses)\n",
    "            tq.set_postfix(loss=mean_loss)\n",
    "\n",
    "            acc = get_accuracy(target, out, device)\n",
    "            accuracy.append(acc)\n",
    "\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    avg_acc = np.array(accuracy).mean(axis=0)\n",
    "    return mean_loss, avg_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007e56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, optimizer, \n",
    "          loss, anchors, strides, start_epoch, epochs, best_loss, name=\"test_name\", device='cuda'):\n",
    "    scaled_anchors = (torch.tensor(anchors) * \n",
    "                     torch.tensor(strides).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2))\n",
    "\n",
    "    scaled_anchors = scaled_anchors.to(device)\n",
    "\n",
    "    best_model = model.state_dict()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "    history['train_acc'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    history['test_loss'] = 0\n",
    "    history['test_acc'] = 0\n",
    "\n",
    "    for epoch in range(1 + start_epoch, 1+epochs):\n",
    "        train_loss, train_acc = train_step(model, train_loader, \n",
    "                                           optimizer, loss, scaled_anchors,\n",
    "                                           scaler, device)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "\n",
    "        val_loss, val_acc = val(model, val_loader, \n",
    "                                optimizer, loss, \n",
    "                                scaled_anchors, scaler, device)\n",
    "\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        if best_loss == best_loss and (best_loss is None or val_loss < best_loss):\n",
    "            best_model = model.state_dict()\n",
    "            best_loss = val_loss\n",
    "    \n",
    "            path = model_dir + \"/\"+ str(epoch) +\"_epoch_model_{:.2f}\".format(val_loss) +'.ph'\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': best_loss,\n",
    "            }, path)\n",
    "    \n",
    "        last_weight_path = model_dir + '/'+ 'last_weight.ph'\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': best_loss,\n",
    "        }, last_weight_path)\n",
    "\n",
    "\n",
    "#     model.load_state_dict(best_model)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45621bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_img_paths))\n",
    "print(len(val_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f72ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "learning_rate = 1e-5\n",
    "weight_decay = 1e-4\n",
    "num_of_epochs = 150\n",
    "anchors = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "strides=[13, 26, 52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1eb731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IDDDataset(train_img_paths, images_path, labels_path,\n",
    "                             anchors, image_size=img_size,\n",
    "                             strides=[13, 26, 52], classes=len(classes),\n",
    "                             transforms=train_transforms)\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "val_dataset = IDDDataset(val_img_paths, images_path, labels_path,\n",
    "                             anchors, image_size=img_size,\n",
    "                             strides=[13, 26, 52], classes=len(classes),\n",
    "                             transforms=test_transforms)\n",
    "val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "start_epoch = 0\n",
    "best_loss = None\n",
    "\n",
    "load =True\n",
    "\n",
    "if load:\n",
    "    checkpoint = torch.load(\"/work/Detection/idd-detection/IDD_Detection/Models/last_weight.ph\")\n",
    "    start_epoch =  checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    best_loss = checkpoint['loss']\n",
    "    \n",
    "\n",
    "history = train(model, train_loader, val_loader, optimizer, \n",
    "          Loss, anchors, strides, start_epoch, num_of_epochs, best_loss, 'w', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0105c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
